{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1729e9b1",
   "metadata": {},
   "source": [
    "<h1> SGD and GD Code </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66805aa1",
   "metadata": {},
   "source": [
    "### (On MNIST) -- Includes comparison plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a68f0",
   "metadata": {},
   "source": [
    "### On bar above, navigate to Cell --> Run All. This will run all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fd5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import ticker\n",
    "import math\n",
    "from math import *\n",
    "import time\n",
    "import random \n",
    "from numpy import linalg as LA\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2efda28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanS\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1) #This is for fetching the MNIST data\n",
    "mnist.keys()\n",
    "U0, v0 = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe6b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)    (70000,)\n"
     ]
    }
   ],
   "source": [
    "U = U0.astype(np.double) # changes the attribute values to float\n",
    "v = v0.astype(np.uint8) # changes labels from string to integer\n",
    "print(U.shape,\"  \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d2dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels \n",
    "#Change the == value to an int inclusive 0 to 9 to binary classify that number\n",
    "v_bin_5_lst = [2*int(v[i]==5)-1 for i in range(len(v))]   #{-1ï¼Œ1} for LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c280091",
   "metadata": {},
   "source": [
    "# Data preparation: train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de9d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single dataframe of all attributes and their labels\n",
    "import pandas as pd\n",
    "df_U = pd.DataFrame(data=U)\n",
    "df_v = pd.DataFrame(data=np.asarray(v_bin_5_lst),  columns=['label'])\n",
    "df_data_merged =pd.concat([df_U, df_v.reindex(df_U.index)], axis=1)\n",
    "#df_data_merged.head(10)\n",
    "#df_data_merged.info()\n",
    "\n",
    "# Split the merged data randomly to train and test\n",
    "def split_train_test(df_data_merged, train_set_size,test_set_size):\n",
    "    np.random.seed(0) # use this line if you want your train and test data remain the same accross experiments\n",
    "    shuffled_indices = np.random.permutation(len(df_data_merged))\n",
    "    \n",
    "    dic_train_set = {}\n",
    "    dic_train_set_indices = shuffled_indices[:train_set_size]\n",
    "    dic_train_set[0] = df_data_merged.iloc[dic_train_set_indices]\n",
    "    \n",
    "    dic_test_set= {}\n",
    "    test_indices = shuffled_indices[-test_set_size:]\n",
    "    dic_test_set[0] = df_data_merged.iloc[test_indices]\n",
    "    \n",
    "    return dic_train_set, dic_test_set\n",
    "\n",
    "# Split the train or test data to U_train and v_train\n",
    "def split_train_or_test_df_to_U_v(train_or_test_df):\n",
    "    npar_data= (train_or_test_df).to_numpy()\n",
    "    nparr_data_transp = npar_data.T\n",
    "    U_output = nparr_data_transp[:-1,:]\n",
    "    v_output =nparr_data_transp[-1:,:]\n",
    "    return U_output,v_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438309de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 1000\n",
      "(784, 1000) (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_set_size,test_set_size = 1000,100  # you can adjust the train size here\n",
    "\n",
    "dic_train_set, dic_test_set = split_train_test(df_data_merged, train_set_size,test_set_size)\n",
    "#dic_train_set[0].head()\n",
    "\n",
    "U_train,v_train = split_train_or_test_df_to_U_v(dic_train_set[0]) #Training Variables\n",
    "n_train , N_train = np.shape(U_train)\n",
    "\n",
    "U_test,v_test = split_train_or_test_df_to_U_v(dic_test_set[0]) #Testing variables\n",
    "n_test , N_test = np.shape(U_test)\n",
    "\n",
    "n = n_train # dimension / features\n",
    "N = N_train # observations / samples\n",
    "\n",
    "print(n,N)\n",
    "print(U_train.shape,v_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7af06",
   "metadata": {},
   "source": [
    "### Here U_train.T would be your $A$, v_train.T would be your $b$.\n",
    "Do $GD$ for at least 2000 iterations, adjust the choice of stepsize (you can start with 0.001) until you get convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc267f4",
   "metadata": {},
   "source": [
    "# Gradient Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9699e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    #np.dot is used to perform matrix multiplication. DON'T use it on scalars\n",
    "    result1 =  sum(localGradient(x, j) for j in range(N))#This is the gradient of an Ax - b function for GD\n",
    "    return result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1e1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, step_size, kmax, epoch):\n",
    "    xnow = x0\n",
    "    value = np.zeros(epoch+1)\n",
    "    value[0] = logisticRegression(xnow)\n",
    "    counter = 1 #Counter for the if statement present in the for loop\n",
    "    for k in range(kmax):\n",
    "        xnow = xnow - step_size * gradient(xnow)\n",
    "        \n",
    "        #This is for collecting data that will be used in the graph\n",
    "        if ((k+1) % math.ceil(kmax / epoch)) == 0:\n",
    "            value[counter] = logisticRegression(xnow) \n",
    "            counter +=1\n",
    "    \n",
    "    \n",
    "    return xnow, value #xnow is final position, value is array of data points for graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4da12",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4dd64d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function that finds the local gradient given a position x and randomized integer j. \n",
    "#For SGD\n",
    "def localGradient(x, j):\n",
    "    result = ((-U_train[:,[j]] * v_train[:,[j]]) / (1 + np.exp(-v_train[:,[j]] * np.dot(U_train.T[[j],:], x)))) / N\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa0fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent function\n",
    "def SGD(x0, step_size, kmax, epoch):\n",
    "    xnow = x0\n",
    "    value = np.zeros(epoch+1)\n",
    "    value[0] = logisticRegression(xnow)\n",
    "    counter = 1 #Counter for if statement in the for loop\n",
    "    for k in range(kmax):\n",
    "        xi = random.randint(0, N-1) #Randomizing a integer for SGD. Recall that N is total digits being tested\n",
    "        xnow = xnow - step_size * localGradient(xnow, xi)\n",
    "            \n",
    "        #This is for collecting data that will be used in the graph\n",
    "        if ((k+1) % math.ceil(kmax / epoch)) == 0:\n",
    "            value[counter] = logisticRegression(xnow) \n",
    "            counter +=1\n",
    "    \n",
    "    \n",
    "    return xnow, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cb699",
   "metadata": {},
   "source": [
    "# Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6b5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(x):\n",
    "    return (sum(np.log(1 + np.exp(-v_train.T[[j],:] * np.dot(U_train.T[[j],:], x))) for j in range(N))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4cc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    return (((np.linalg.norm(np.dot(A, x) - b)) ** 2) / (2 * N))\n",
    "\n",
    "#This is the objective function we use for minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8616361",
   "metadata": {},
   "source": [
    "# Running SGD or GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5fa43",
   "metadata": {},
   "source": [
    "### Make sure to run GD before SGD because GD sets random seed, A, and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced8118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Change the random seed here.\n",
    "np.random.seed(0)\n",
    "\n",
    "#A = U_train.T #Setting A as pixel data\n",
    "#b = v_train.T #Setting b as the -1 or 1 array. Reference the 'change labels' cell.\n",
    "\n",
    "#Adjust stepsize until you get convergence\n",
    "ss1 = 10 ** -7\n",
    "\n",
    "#Number of iterations\n",
    "kmax1 = 100\n",
    "\n",
    "#Initial xnow point\n",
    "x1 = np.zeros((784, 1))\n",
    "\n",
    "#Number of data points -1\n",
    "ep = 10\n",
    "\n",
    "#Final1 is final xnow after performing GD, graphData1 is used for plots in next section\n",
    "final1, graphData1 = gradient_descent(x1, ss1, kmax1, ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f0512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Adjust stepsize until you get convergence\n",
    "ss2 = 10 ** -7\n",
    "\n",
    "#Number of iterations\n",
    "kmax2 = kmax1 * N\n",
    "\n",
    "#Initial xnow point\n",
    "x2 = np.zeros((784, 1))\n",
    "\n",
    "#Number of data points - 1\n",
    "ep2 = 10\n",
    "\n",
    "#Final2 is final xnow after performing SGD, graphData2 is used for plots in next section\n",
    "final2, graphData2 = SGD(x2, ss2, kmax2, ep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422e069",
   "metadata": {},
   "source": [
    "<h1> Graphs <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050becc4",
   "metadata": {},
   "source": [
    "# Data for GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure you don't get 'nan' values!\n",
    "print(final1) #Final x array after performing GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4d721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "\n",
    "#This is the line code\n",
    "plt.plot(range(0,kmax1+1, math.ceil(kmax1 / ep)), graphData1, color='b',\n",
    "         marker='X',markersize=10,linestyle='dashed',label=\"Gradient-Descent\",linewidth=3.5)\n",
    "\n",
    "#General formatting code\n",
    "plt.legend(loc=1,fontsize=25)\n",
    "plt.xlabel('Total communication rounds', color='#1C2833',fontsize=30)\n",
    "plt.ylabel('Log of average loss', color='#1C2833',fontsize=30)\n",
    "plt.xticks(range(0,kmax1+1, math.ceil(kmax1 / ep)),fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.margins(0.022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67860bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graphData1[-1]) #Checking the value for the last data point. Make sure this is not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a4f5e",
   "metadata": {},
   "source": [
    "# Data for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f7e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make sure you don't get 'nan' values!\n",
    "print(final2)#Final x array after performing SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66643694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(range(0,kmax2+1, math.ceil(kmax2 / ep)), graphData2, color='b',\n",
    "         marker='X',markersize=10,linestyle='dashed',label=\"SGD\",linewidth=3.5)\n",
    "\n",
    "#General formatting code\n",
    "plt.legend(loc=1,fontsize=25)\n",
    "plt.xlabel('Total communication rounds', color='#1C2833',fontsize=30)\n",
    "plt.ylabel('Log of average loss', color='#1C2833',fontsize=30)\n",
    "plt.xticks(range(0,kmax2+1, math.ceil(kmax2 / ep)),fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.margins(0.022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb02f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(graphData2[-1]) #Checking the value of the last data point. Make sure this is not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57e9b9",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfba5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This graph compares the performance of SGD and Gradient Descent\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "#Make sure all kmax inputs are kmax2.\n",
    "\n",
    "#SGD line\n",
    "plt.plot(range(0,kmax2+1, math.ceil(kmax2 / ep)), graphData2, color='b',\n",
    "         marker='X',markersize=10,linestyle='dashed',label=\"SGD\",linewidth=3.5)\n",
    "\n",
    "#GD line\n",
    "plt.plot(range(0,kmax2+1, math.ceil(kmax2 / ep)), graphData1, color='g',\n",
    "         marker='X',markersize=10,linestyle='dashed',label=\"Gradient-Descent\",linewidth=3.5)\n",
    "\n",
    "plt.legend(loc=1,fontsize=25)\n",
    "plt.xlabel('Number of Gradient Evaluations', color='#1C2833',fontsize=30)\n",
    "plt.ylabel('Objective Function Value', color='#1C2833',fontsize=30)\n",
    "plt.xticks(np.linspace(0, kmax2, 6),fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.margins(0.022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be22b0",
   "metadata": {},
   "source": [
    "# Function for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba501e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confu_mat(opt_sol):\n",
    "    pred_labels = np.zeros((N_test,1))\n",
    "    for j in range(N_test):\n",
    "        pred_labels[j][0] = np.sign(np.dot(U_test[:,[j]].T,opt_sol))\n",
    "    return confusion_matrix(v_test.T, pred_labels)\n",
    "\n",
    "def precision_score(opt_sol):\n",
    "    confu_mat = my_confu_mat(opt_sol)\n",
    "    return (confu_mat[0][0]+confu_mat[1][1])/(confu_mat[1][0]+confu_mat[0][1]+confu_mat[1][1]+confu_mat[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c17bd1",
   "metadata": {},
   "source": [
    "# Testing our Descent Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Gradient Descent Precision Score\n",
    "ps_GD = precision_score(final1) \n",
    "\n",
    "print(\"Test score for GD: \", \"{0:.2%}\".format(ps_GD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb78aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test Stochastic Gradient Descent Precision Score\n",
    "ps_SGD = precision_score(final2) \n",
    "\n",
    "print(\"Test score for SGD: \", \"{0:.2%}\".format(ps_SGD))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
